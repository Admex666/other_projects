{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import schedule\n",
    "import threading\n",
    "from typing import Dict, List, Optional\n",
    "import re\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "# Logging be√°ll√≠t√°sa\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SofaScoreScraper:\n",
    "    def __init__(self, headless: bool = True):\n",
    "        self.headless = headless\n",
    "        self.driver = None\n",
    "        self.session = requests.Session()\n",
    "        self.base_headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "        self.session.headers.update(self.base_headers)\n",
    "        \n",
    "    def setup_selenium(self):\n",
    "        \"\"\"Selenium WebDriver be√°ll√≠t√°sa\"\"\"\n",
    "        try:\n",
    "            chrome_options = Options()\n",
    "            if self.headless:\n",
    "                chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--disable-gpu\")\n",
    "            chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "            chrome_options.add_argument(f\"--user-agent={self.base_headers['User-Agent']}\")\n",
    "            \n",
    "            # Felh≈ë k√∂rnyezethez optimaliz√°lt be√°ll√≠t√°sok\n",
    "            chrome_options.add_argument(\"--disable-extensions\")\n",
    "            chrome_options.add_argument(\"--disable-plugins\")\n",
    "            chrome_options.add_argument(\"--disable-images\")\n",
    "            \n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            logger.info(\"Selenium WebDriver successfully initialized\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to setup Selenium: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def extract_match_id_from_url(self, url: str) -> Optional[str]:\n",
    "        \"\"\"Match ID kinyer√©se az URL-b≈ël\"\"\"\n",
    "        try:\n",
    "            # SofaScore URL pattern: /match/team1-team2/abc#id:12345,tab:statistics\n",
    "            if '#id:' in url:\n",
    "                match_id = url.split('#id:')[1].split(',')[0]\n",
    "                return match_id\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting match ID: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_with_requests(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"Pr√≥b√°lkoz√°s requests-tel (gyorsabb, de nem mindig m≈±k√∂dik)\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'lxml')\n",
    "            return self.parse_statistics(soup)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Requests method failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_with_selenium(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"Selenium haszn√°lata dinamikus tartalom bet√∂lt√©s√©hez\"\"\"\n",
    "        try:\n",
    "            if not self.driver and not self.setup_selenium():\n",
    "                return None\n",
    "            \n",
    "            self.driver.get(url)\n",
    "            \n",
    "            # V√°runk, hogy bet√∂lt≈ëdj√∂n az oldal\n",
    "            wait = WebDriverWait(self.driver, 20)\n",
    "            \n",
    "            # T√∂bb lehets√©ges selector pr√≥b√°l√°sa\n",
    "            selectors_to_try = [\n",
    "                \".pt_sm.bdr-b_lg.ov_hidden\",\n",
    "                \"[data-testid='statistics']\",\n",
    "                \".statistics\",\n",
    "                \"[class*='statistics']\",\n",
    "                \"[class*='stat']\"\n",
    "            ]\n",
    "            \n",
    "            stats_container = None\n",
    "            for selector in selectors_to_try:\n",
    "                try:\n",
    "                    stats_container = wait.until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "                    )\n",
    "                    logger.info(f\"Found stats container with selector: {selector}\")\n",
    "                    break\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            if not stats_container:\n",
    "                logger.warning(\"No statistics container found with any selector\")\n",
    "                # Pr√≥b√°ljuk meg a teljes oldal parsing-ot\n",
    "                time.sleep(5)  # Tov√°bbi v√°rakoz√°s\n",
    "            else:\n",
    "                time.sleep(3)  # Statisztik√°k bet√∂lt≈ëd√©s√©re v√°rakoz√°s\n",
    "            \n",
    "            # HTML tartalom lek√©r√©se √©s debug info\n",
    "            page_source = self.driver.page_source\n",
    "            \n",
    "            # Debug: megn√©zz√ºk mi van az oldalon\n",
    "            soup = BeautifulSoup(page_source, 'lxml')\n",
    "            logger.info(f\"Page title: {soup.title.string if soup.title else 'No title'}\")\n",
    "            \n",
    "            # Keres√ºnk b√°rmilyen div-et ami statisztik√°ra utalhat\n",
    "            potential_stats = soup.find_all('div', string=re.compile(r'xG|Expected|Goals|Shots|Possession', re.I))\n",
    "            logger.info(f\"Found {len(potential_stats)} potential stat elements\")\n",
    "            \n",
    "            return self.parse_statistics(soup)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Selenium scraping failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_statistics(self, soup: BeautifulSoup) -> Dict:\n",
    "        \"\"\"Statisztik√°k kinyer√©se a HTML-b≈ël - jav√≠tott verzi√≥ t√∂bb selector-ral\"\"\"\n",
    "        stats = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'team_stats': {},\n",
    "            'xg_data': {},\n",
    "            'raw_stats': []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Debug: n√©zz√ºk meg mi van az oldalon\n",
    "            logger.info(\"Starting to parse statistics...\")\n",
    "            \n",
    "            # T√∂bb lehets√©ges statisztika kont√©ner keres√©se\n",
    "            container_selectors = [\n",
    "                'div.pt_sm.bdr-b_lg.ov_hidden',\n",
    "                '[data-testid*=\"stat\"]',\n",
    "                '.statistics-container',\n",
    "                '[class*=\"statistics\"]',\n",
    "                '[class*=\"stat-row\"]',\n",
    "                'div[class*=\"flex\"]'  # SofaScore gyakran flexbox-ot haszn√°l\n",
    "            ]\n",
    "            \n",
    "            stats_containers = []\n",
    "            for selector in container_selectors:\n",
    "                containers = soup.select(selector)\n",
    "                if containers:\n",
    "                    stats_containers.extend(containers)\n",
    "                    logger.info(f\"Found {len(containers)} containers with selector: {selector}\")\n",
    "            \n",
    "            # Ha nincs specifikus kont√©ner, keres√ºnk az eg√©sz oldalon\n",
    "            if not stats_containers:\n",
    "                logger.info(\"No specific containers found, searching entire page...\")\n",
    "                stats_containers = [soup]\n",
    "            \n",
    "            # Statisztik√°k keres√©se k√ºl√∂nb√∂z≈ë m√≥dszerekkel\n",
    "            for container in stats_containers:\n",
    "                # 1. m√≥dszer: Flexbox alap√∫ keres√©s\n",
    "                self._parse_flexbox_stats(container, stats)\n",
    "                \n",
    "                # 2. m√≥dszer: Sz√∂veg alap√∫ keres√©s\n",
    "                self._parse_text_based_stats(container, stats)\n",
    "                \n",
    "                # 3. m√≥dszer: Table-szer≈± strukt√∫ra\n",
    "                self._parse_table_like_stats(container, stats)\n",
    "            \n",
    "            # xG specifikus keres√©s\n",
    "            self._extract_xg_data(soup, stats)\n",
    "            \n",
    "            # Csapat nevek kinyer√©se\n",
    "            team_names = self.extract_team_names(soup)\n",
    "            if team_names:\n",
    "                stats['teams'] = team_names\n",
    "            \n",
    "            logger.info(f\"Successfully parsed {len(stats['raw_stats'])} statistics\")\n",
    "            \n",
    "            # Debug: logoljuk az els≈ë p√°r statisztik√°t\n",
    "            for i, stat in enumerate(stats['raw_stats'][:3]):\n",
    "                logger.info(f\"Stat {i+1}: {stat}\")\n",
    "            \n",
    "            return stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error parsing statistics: {e}\")\n",
    "            return stats\n",
    "    \n",
    "    def _parse_flexbox_stats(self, container, stats):\n",
    "        \"\"\"Flexbox alap√∫ statisztik√°k parsing\"\"\"\n",
    "        try:\n",
    "            # Keres√ºnk sorokra osztott statisztik√°kat\n",
    "            rows = container.find_all('div', class_=re.compile(r'.*flex.*|.*row.*'))\n",
    "            \n",
    "            for row in rows:\n",
    "                text_elements = row.find_all(string=True)\n",
    "                text_content = [t.strip() for t in text_elements if t.strip()]\n",
    "                \n",
    "                # Ha 3 elemet tal√°lunk: √©rt√©k1, n√©v, √©rt√©k2\n",
    "                if len(text_content) >= 3:\n",
    "                    # Keres√ºnk sz√°mokat a sz√∂vegben\n",
    "                    numbers = [t for t in text_content if re.match(r'^[\\d.,]+%?$', t)]\n",
    "                    stat_names = [t for t in text_content if not re.match(r'^[\\d.,]+%?$', t) and len(t) > 1]\n",
    "                    \n",
    "                    if len(numbers) >= 2 and stat_names:\n",
    "                        stat_name = stat_names[0]\n",
    "                        stats['raw_stats'].append({\n",
    "                            'stat_name': stat_name,\n",
    "                            'home_value': numbers[0],\n",
    "                            'away_value': numbers[-1]\n",
    "                        })\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error in flexbox parsing: {e}\")\n",
    "    \n",
    "    def _parse_text_based_stats(self, container, stats):\n",
    "        \"\"\"Sz√∂veg alap√∫ statisztika keres√©s\"\"\"\n",
    "        try:\n",
    "            # Keress√ºk az √∂sszes sz√∂veget ami statisztik√°ra utalhat\n",
    "            stat_patterns = [\n",
    "                r'(\\d+[\\.,]?\\d*)\\s*(xG|Expected goals?|Goals?|Shots?|Possession|Pass|Corner|Yellow|Red)',\n",
    "                r'(xG|Expected goals?|Goals?|Shots?|Possession|Pass|Corner|Yellow|Red)\\s*(\\d+[\\.,]?\\d*)',\n",
    "                r'(\\d+[\\.,]?\\d*%?)\\s*([A-Za-z\\s]+)\\s*(\\d+[\\.,]?\\d*%?)'\n",
    "            ]\n",
    "            \n",
    "            page_text = container.get_text()\n",
    "            \n",
    "            for pattern in stat_patterns:\n",
    "                matches = re.finditer(pattern, page_text, re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    groups = match.groups()\n",
    "                    if len(groups) >= 2:\n",
    "                        # Pr√≥b√°ljuk meghat√°rozni melyik a n√©v √©s melyik az √©rt√©k\n",
    "                        if groups[1].replace(' ', '').replace('.', '').replace(',', '').isdigit():\n",
    "                            stat_name = groups[0]\n",
    "                            value = groups[1]\n",
    "                        else:\n",
    "                            stat_name = groups[1]\n",
    "                            value = groups[0]\n",
    "                        \n",
    "                        if stat_name and value:\n",
    "                            stats['raw_stats'].append({\n",
    "                                'stat_name': stat_name,\n",
    "                                'home_value': value,\n",
    "                                'away_value': 'N/A'\n",
    "                            })\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error in text-based parsing: {e}\")\n",
    "    \n",
    "    def _parse_table_like_stats(self, container, stats):\n",
    "        \"\"\"Table-szer≈± strukt√∫r√°k parsing\"\"\"\n",
    "        try:\n",
    "            # Keres√ºnk tr, td elemeket\n",
    "            rows = container.find_all(['tr', 'div'])\n",
    "            \n",
    "            for row in rows:\n",
    "                cells = row.find_all(['td', 'div', 'span'])\n",
    "                if len(cells) >= 3:\n",
    "                    cell_texts = [cell.get_text(strip=True) for cell in cells]\n",
    "                    \n",
    "                    # Sz≈±rj√ºk ki az √ºres cell√°kat\n",
    "                    cell_texts = [text for text in cell_texts if text]\n",
    "                    \n",
    "                    if len(cell_texts) >= 3:\n",
    "                        # √Åltal√°ban: √©rt√©k1, statisztika_n√©v, √©rt√©k2\n",
    "                        potential_left = cell_texts[0]\n",
    "                        potential_name = cell_texts[1]\n",
    "                        potential_right = cell_texts[-1]\n",
    "                        \n",
    "                        # Ellen≈ërizz√ºk hogy van-e sz√°m√©rt√©k\n",
    "                        if (re.match(r'^[\\d.,]+%?$', potential_left) and \n",
    "                            re.match(r'^[\\d.,]+%?$', potential_right) and\n",
    "                            not re.match(r'^[\\d.,]+%?$', potential_name)):\n",
    "                            \n",
    "                            stats['raw_stats'].append({\n",
    "                                'stat_name': potential_name,\n",
    "                                'home_value': potential_left,\n",
    "                                'away_value': potential_right\n",
    "                            })\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error in table-like parsing: {e}\")\n",
    "    \n",
    "    def _extract_xg_data(self, soup, stats):\n",
    "        \"\"\"xG adatok specifikus kinyer√©se\"\"\"\n",
    "        try:\n",
    "            # xG keres√©se k√ºl√∂nb√∂z≈ë m√≥dokon\n",
    "            xg_patterns = [\n",
    "                r'xG[:\\s]*(\\d+[\\.,]?\\d*)',\n",
    "                r'Expected\\s+goals?[:\\s]*(\\d+[\\.,]?\\d*)',\n",
    "                r'(\\d+[\\.,]?\\d*)\\s*xG'\n",
    "            ]\n",
    "            \n",
    "            page_text = soup.get_text()\n",
    "            \n",
    "            xg_values = []\n",
    "            for pattern in xg_patterns:\n",
    "                matches = re.findall(pattern, page_text, re.IGNORECASE)\n",
    "                xg_values.extend(matches)\n",
    "            \n",
    "            if len(xg_values) >= 2:\n",
    "                stats['xg_data'] = {\n",
    "                    'home_xg': xg_values[0],\n",
    "                    'away_xg': xg_values[1],\n",
    "                    'stat_name': 'xG'\n",
    "                }\n",
    "                logger.info(f\"Found xG data: {stats['xg_data']}\")\n",
    "            elif len(xg_values) == 1:\n",
    "                stats['xg_data'] = {\n",
    "                    'home_xg': xg_values[0],\n",
    "                    'away_xg': 'N/A',\n",
    "                    'stat_name': 'xG'\n",
    "                }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error extracting xG data: {e}\")\n",
    "\n",
    "\n",
    "    def extract_team_names(self, soup: BeautifulSoup) -> Optional[Dict]:\n",
    "        \"\"\"Csapat nevek kinyer√©se\"\"\"\n",
    "        try:\n",
    "            # K√ºl√∂nb√∂z≈ë selectorok pr√≥b√°l√°sa\n",
    "            selectors = [\n",
    "                'h1[data-testid=\"match-header-team-name\"]',\n",
    "                '.team-name',\n",
    "                '[class*=\"team\"][class*=\"name\"]',\n",
    "                'h1, h2, h3'  # Fallback\n",
    "            ]\n",
    "            \n",
    "            for selector in selectors:\n",
    "                elements = soup.select(selector)\n",
    "                if len(elements) >= 2:\n",
    "                    return {\n",
    "                        'home_team': elements[0].get_text(strip=True),\n",
    "                        'away_team': elements[1].get_text(strip=True)\n",
    "                    }\n",
    "            \n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting team names: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_match_stats(self, url: str) -> Optional[Dict]:\n",
    "        \"\"\"F≈ë scraping f√ºggv√©ny - el≈ësz√∂r requests, majd Selenium\"\"\"\n",
    "        logger.info(f\"Starting to scrape: {url}\")\n",
    "        \n",
    "        # El≈ësz√∂r pr√≥b√°lkoz√°s requests-tel\n",
    "        stats = self.scrape_with_requests(url)\n",
    "        \n",
    "        # Ha nem siker√ºlt, Selenium haszn√°lata\n",
    "        if not stats or len(stats.get('raw_stats', [])) == 0:\n",
    "            logger.info(\"Trying with Selenium...\")\n",
    "            stats = self.scrape_with_selenium(url)\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Er≈ëforr√°sok felszabad√≠t√°sa\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            self.driver = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchScheduler:\n",
    "    def __init__(self, scraper: SofaScoreScraper):\n",
    "        self.scraper = scraper\n",
    "        self.results = []\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        \n",
    "    def add_match(self, url: str, interval_minutes: int = 5):\n",
    "        \"\"\"M√©rk≈ëz√©s hozz√°ad√°sa az √ºtemez√©shez\"\"\"\n",
    "        def job():\n",
    "            try:\n",
    "                stats = self.scraper.scrape_match_stats(url)\n",
    "                if stats:\n",
    "                    stats['url'] = url\n",
    "                    self.results.append(stats)\n",
    "                    logger.info(f\"Successfully scraped match data. Total results: {len(self.results)}\")\n",
    "                else:\n",
    "                    logger.warning(\"No stats retrieved\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in scheduled job: {e}\")\n",
    "        \n",
    "        schedule.every(interval_minutes).minutes.do(job)\n",
    "        logger.info(f\"Scheduled match scraping every {interval_minutes} minutes for: {url}\")\n",
    "        \n",
    "        # Els≈ë futtat√°s azonnal\n",
    "        job()\n",
    "    \n",
    "    def start_scheduler(self):\n",
    "        \"\"\"√útemez≈ë ind√≠t√°sa k√ºl√∂n sz√°lon\"\"\"\n",
    "        def run_scheduler():\n",
    "            self.running = True\n",
    "            while self.running:\n",
    "                schedule.run_pending()\n",
    "                time.sleep(1)\n",
    "        \n",
    "        self.thread = threading.Thread(target=run_scheduler, daemon=True)\n",
    "        self.thread.start()\n",
    "        logger.info(\"Scheduler started\")\n",
    "    \n",
    "    def stop_scheduler(self):\n",
    "        \"\"\"√útemez≈ë le√°ll√≠t√°sa\"\"\"\n",
    "        self.running = False\n",
    "        schedule.clear()\n",
    "        logger.info(\"Scheduler stopped\")\n",
    "    \n",
    "    def get_latest_stats(self) -> Optional[Dict]:\n",
    "        \"\"\"Legfrissebb statisztik√°k lek√©r√©se\"\"\"\n",
    "        if self.results:\n",
    "            return self.results[-1]\n",
    "        return None\n",
    "    \n",
    "    def save_results_to_csv(self, filename: str = None):\n",
    "        \"\"\"Eredm√©nyek ment√©se CSV-be\"\"\"\n",
    "        if not self.results:\n",
    "            logger.warning(\"No results to save\")\n",
    "            return\n",
    "        \n",
    "        if not filename:\n",
    "            filename = f\"sofascore_stats_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        \n",
    "        # DataFrame k√©sz√≠t√©se\n",
    "        rows = []\n",
    "        for result in self.results:\n",
    "            base_row = {\n",
    "                'timestamp': result.get('timestamp'),\n",
    "                'url': result.get('url'),\n",
    "                'home_team': result.get('teams', {}).get('home_team', 'Unknown'),\n",
    "                'away_team': result.get('teams', {}).get('away_team', 'Unknown'),\n",
    "                'home_xg': result.get('xg_data', {}).get('home_xg', 'N/A'),\n",
    "                'away_xg': result.get('xg_data', {}).get('away_xg', 'N/A')\n",
    "            }\n",
    "            \n",
    "            # Egy√©b statisztik√°k hozz√°ad√°sa\n",
    "            for stat in result.get('raw_stats', []):\n",
    "                base_row[f\"home_{stat['stat_name']}\"] = stat['home_value']\n",
    "                base_row[f\"away_{stat['stat_name']}\"] = stat['away_value']\n",
    "            \n",
    "            rows.append(base_row)\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(filename, index=False)\n",
    "        logger.info(f\"Results saved to {filename}\")\n",
    "        return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:18:42,563 - INFO - Starting to scrape: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:18:42,650 - WARNING - Requests method failed: 403 Client Error: Forbidden for url: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:18:42,651 - INFO - Trying with Selenium...\n",
      "2025-07-25 18:18:42,652 - INFO - ====== WebDriver manager ======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèà Testing single match scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:18:43,610 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-07-25 18:18:43,700 - INFO - Get LATEST chromedriver version for google-chrome\n",
      "2025-07-25 18:18:43,765 - INFO - Driver [C:\\Users\\Adam\\.wdm\\drivers\\chromedriver\\win64\\138.0.7204.168\\chromedriver-win32/chromedriver.exe] found in cache\n",
      "2025-07-25 18:18:44,824 - INFO - Selenium WebDriver successfully initialized\n",
      "2025-07-25 18:18:46,963 - INFO - Found stats container with selector: .pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:18:50,248 - INFO - Page title: 1. FC K√∂ln vs Leicester City live score, H2H and lineups | Sofascore\n",
      "2025-07-25 18:18:50,256 - INFO - Found 6 potential stat elements\n",
      "2025-07-25 18:18:50,257 - INFO - Starting to parse statistics...\n",
      "2025-07-25 18:18:50,283 - INFO - Found 1 containers with selector: div.pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:18:50,329 - INFO - Found 1 containers with selector: [data-testid*=\"stat\"]\n",
      "2025-07-25 18:18:50,509 - INFO - Found 253 containers with selector: div[class*=\"flex\"]\n",
      "2025-07-25 18:18:50,824 - INFO - Successfully parsed 354 statistics\n",
      "2025-07-25 18:18:50,825 - INFO - Stat 1: {'stat_name': 'Match overview', 'home_value': '48%', 'away_value': '0'}\n",
      "2025-07-25 18:18:50,825 - INFO - Stat 2: {'stat_name': 'Ball possession', 'home_value': '48%', 'away_value': '52%'}\n",
      "2025-07-25 18:18:50,825 - INFO - Stat 3: {'stat_name': 'Corner kicks', 'home_value': '2', 'away_value': '3'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully scraped statistics!\n",
      "üìä Found 354 statistics\n",
      "üèüÔ∏è Teams: {'home_team': '1. FC K√∂lnvsLeicester Citylive score, H2H results, standings and prediction', 'away_team': 'Lineups'}\n",
      "\n",
      "üìà Sample statistics:\n",
      "  Match overview: 48% - 0\n",
      "  Ball possession: 48% - 52%\n",
      "  Corner kicks: 2 - 3\n",
      "  Fouls: 9 - 6\n",
      "  Free kicks: 6 - 9\n"
     ]
    }
   ],
   "source": [
    "# Haszn√°lati p√©lda √©s tesztel√©s\n",
    "\n",
    "# Scraper inicializ√°l√°s\n",
    "scraper = SofaScoreScraper(headless=True)  # headless=False a tesztel√©shez\n",
    "\n",
    "# Tesztel√©s egyedi URL-lel\n",
    "test_url = \"https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\"\n",
    "\n",
    "print(\"üèà Testing single match scraping...\")\n",
    "stats = scraper.scrape_match_stats(test_url)\n",
    "\n",
    "if stats:\n",
    "    print(f\"‚úÖ Successfully scraped statistics!\")\n",
    "    print(f\"üìä Found {len(stats.get('raw_stats', []))} statistics\")\n",
    "    \n",
    "    if stats.get('xg_data'):\n",
    "        print(f\"‚öΩ xG found: {stats['xg_data']}\")\n",
    "    else:\n",
    "        print(\"‚ùå No xG data found\")\n",
    "    \n",
    "    xg_stats = [s for s in stats['raw_stats'] if 'xG' in s['stat_name'] or 'Expected' in s['stat_name']]\n",
    "    print(f\"üéØ xG related stats: {xg_stats}\")\n",
    "\n",
    "    if stats.get('teams'):\n",
    "        print(f\"üèüÔ∏è Teams: {stats['teams']}\")\n",
    "    \n",
    "    # Els≈ë n√©h√°ny statisztika megjelen√≠t√©se\n",
    "    print(\"\\nüìà Sample statistics:\")\n",
    "    for stat in stats.get('raw_stats', [])[:5]:\n",
    "        print(f\"  {stat['stat_name']}: {stat['home_value']} - {stat['away_value']}\")\n",
    "else:\n",
    "    print(\"‚ùå Failed to scrape statistics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:23:48,875 - INFO - Scheduled match scraping every 1 minutes for: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:23:48,876 - INFO - Starting to scrape: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:23:48,963 - WARNING - Requests method failed: 403 Client Error: Forbidden for url: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:23:48,964 - INFO - Trying with Selenium...\n",
      "2025-07-25 18:23:48,989 - INFO - Found stats container with selector: .pt_sm.bdr-b_lg.ov_hidden\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üïê Setting up scheduled scraping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:23:52,272 - INFO - Page title: 1. FC K√∂ln vs Leicester City live score, H2H and lineups | Sofascore\n",
      "2025-07-25 18:23:52,280 - INFO - Found 6 potential stat elements\n",
      "2025-07-25 18:23:52,283 - INFO - Starting to parse statistics...\n",
      "2025-07-25 18:23:52,320 - INFO - Found 1 containers with selector: div.pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:23:52,366 - INFO - Found 1 containers with selector: [data-testid*=\"stat\"]\n",
      "2025-07-25 18:23:52,542 - INFO - Found 253 containers with selector: div[class*=\"flex\"]\n",
      "2025-07-25 18:23:52,855 - INFO - Successfully parsed 354 statistics\n",
      "2025-07-25 18:23:52,856 - INFO - Stat 1: {'stat_name': 'Match overview', 'home_value': '48%', 'away_value': '0'}\n",
      "2025-07-25 18:23:52,857 - INFO - Stat 2: {'stat_name': 'Ball possession', 'home_value': '48%', 'away_value': '52%'}\n",
      "2025-07-25 18:23:52,857 - INFO - Stat 3: {'stat_name': 'Corner kicks', 'home_value': '2', 'away_value': '3'}\n",
      "2025-07-25 18:23:52,858 - INFO - Successfully scraped match data. Total results: 1\n",
      "2025-07-25 18:23:52,859 - INFO - Scheduler started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Scheduler started! It will scrape every 1 minutes.\n",
      "üí° Run the next cell to check results after a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:24:48,888 - INFO - Starting to scrape: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:24:48,985 - WARNING - Requests method failed: 403 Client Error: Forbidden for url: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:24:48,985 - INFO - Trying with Selenium...\n",
      "2025-07-25 18:24:49,020 - INFO - Found stats container with selector: .pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:24:52,687 - INFO - Page title: 1. FC K√∂ln vs Leicester City live score, H2H and lineups | Sofascore\n",
      "2025-07-25 18:24:52,696 - INFO - Found 6 potential stat elements\n",
      "2025-07-25 18:24:52,697 - INFO - Starting to parse statistics...\n",
      "2025-07-25 18:24:52,726 - INFO - Found 1 containers with selector: div.pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:24:52,778 - INFO - Found 1 containers with selector: [data-testid*=\"stat\"]\n",
      "2025-07-25 18:24:52,971 - INFO - Found 253 containers with selector: div[class*=\"flex\"]\n",
      "2025-07-25 18:24:53,296 - INFO - Successfully parsed 354 statistics\n",
      "2025-07-25 18:24:53,297 - INFO - Stat 1: {'stat_name': 'Match overview', 'home_value': '49%', 'away_value': '0'}\n",
      "2025-07-25 18:24:53,298 - INFO - Stat 2: {'stat_name': 'Ball possession', 'home_value': '49%', 'away_value': '51%'}\n",
      "2025-07-25 18:24:53,298 - INFO - Stat 3: {'stat_name': 'Corner kicks', 'home_value': '2', 'away_value': '3'}\n",
      "2025-07-25 18:24:53,299 - INFO - Successfully scraped match data. Total results: 2\n",
      "2025-07-25 18:25:54,333 - INFO - Starting to scrape: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:25:54,426 - WARNING - Requests method failed: 403 Client Error: Forbidden for url: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:25:54,427 - INFO - Trying with Selenium...\n",
      "2025-07-25 18:25:54,446 - INFO - Found stats container with selector: .pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:25:58,283 - INFO - Page title: 1. FC K√∂ln vs Leicester City live score, H2H and lineups | Sofascore\n",
      "2025-07-25 18:25:58,291 - INFO - Found 6 potential stat elements\n",
      "2025-07-25 18:25:58,291 - INFO - Starting to parse statistics...\n",
      "2025-07-25 18:25:58,320 - INFO - Found 1 containers with selector: div.pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:25:58,369 - INFO - Found 1 containers with selector: [data-testid*=\"stat\"]\n",
      "2025-07-25 18:25:58,554 - INFO - Found 263 containers with selector: div[class*=\"flex\"]\n",
      "2025-07-25 18:25:58,881 - INFO - Successfully parsed 354 statistics\n",
      "2025-07-25 18:25:58,881 - INFO - Stat 1: {'stat_name': 'Match overview', 'home_value': '49%', 'away_value': '0'}\n",
      "2025-07-25 18:25:58,882 - INFO - Stat 2: {'stat_name': 'Ball possession', 'home_value': '49%', 'away_value': '51%'}\n",
      "2025-07-25 18:25:58,882 - INFO - Stat 3: {'stat_name': 'Corner kicks', 'home_value': '2', 'away_value': '4'}\n",
      "2025-07-25 18:25:58,883 - INFO - Successfully scraped match data. Total results: 5\n",
      "2025-07-25 18:26:48,915 - INFO - Starting to scrape: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:26:48,997 - WARNING - Requests method failed: 403 Client Error: Forbidden for url: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:26:48,998 - INFO - Trying with Selenium...\n",
      "2025-07-25 18:26:49,018 - INFO - Found stats container with selector: .pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:26:52,331 - INFO - Page title: 1. FC K√∂ln vs Leicester City live score, H2H and lineups | Sofascore\n",
      "2025-07-25 18:26:52,339 - INFO - Found 6 potential stat elements\n",
      "2025-07-25 18:26:52,340 - INFO - Starting to parse statistics...\n",
      "2025-07-25 18:26:52,367 - INFO - Found 1 containers with selector: div.pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:26:52,418 - INFO - Found 1 containers with selector: [data-testid*=\"stat\"]\n",
      "2025-07-25 18:26:52,619 - INFO - Found 263 containers with selector: div[class*=\"flex\"]\n",
      "2025-07-25 18:26:52,945 - INFO - Successfully parsed 354 statistics\n",
      "2025-07-25 18:26:52,946 - INFO - Stat 1: {'stat_name': 'Match overview', 'home_value': '49%', 'away_value': '0'}\n",
      "2025-07-25 18:26:52,946 - INFO - Stat 2: {'stat_name': 'Ball possession', 'home_value': '49%', 'away_value': '51%'}\n",
      "2025-07-25 18:26:52,947 - INFO - Stat 3: {'stat_name': 'Corner kicks', 'home_value': '2', 'away_value': '4'}\n",
      "2025-07-25 18:26:52,948 - INFO - Successfully scraped match data. Total results: 2\n",
      "2025-07-25 18:26:58,952 - INFO - Starting to scrape: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:26:59,060 - WARNING - Requests method failed: 403 Client Error: Forbidden for url: https://www.sofascore.com/football/match/1-fc-koln-leicester-city/Gswdb#id:14250691,tab:statistics\n",
      "2025-07-25 18:26:59,060 - INFO - Trying with Selenium...\n",
      "2025-07-25 18:26:59,081 - INFO - Found stats container with selector: .pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:27:02,361 - INFO - Page title: 1. FC K√∂ln vs Leicester City live score, H2H and lineups | Sofascore\n",
      "2025-07-25 18:27:02,369 - INFO - Found 6 potential stat elements\n",
      "2025-07-25 18:27:02,369 - INFO - Starting to parse statistics...\n",
      "2025-07-25 18:27:02,397 - INFO - Found 1 containers with selector: div.pt_sm.bdr-b_lg.ov_hidden\n",
      "2025-07-25 18:27:02,444 - INFO - Found 1 containers with selector: [data-testid*=\"stat\"]\n",
      "2025-07-25 18:27:02,627 - INFO - Found 263 containers with selector: div[class*=\"flex\"]\n",
      "2025-07-25 18:27:02,951 - INFO - Successfully parsed 354 statistics\n",
      "2025-07-25 18:27:02,952 - INFO - Stat 1: {'stat_name': 'Match overview', 'home_value': '49%', 'away_value': '0'}\n",
      "2025-07-25 18:27:02,952 - INFO - Stat 2: {'stat_name': 'Ball possession', 'home_value': '49%', 'away_value': '51%'}\n",
      "2025-07-25 18:27:02,953 - INFO - Stat 3: {'stat_name': 'Corner kicks', 'home_value': '2', 'away_value': '4'}\n",
      "2025-07-25 18:27:02,953 - INFO - Successfully scraped match data. Total results: 6\n"
     ]
    }
   ],
   "source": [
    "# √útemezett scraping p√©lda\n",
    "\n",
    "print(\"\\nüïê Setting up scheduled scraping...\")\n",
    "scheduler = MatchScheduler(scraper)\n",
    "\n",
    "# M√©rk≈ëz√©s hozz√°ad√°sa 2 perces intervallumal (tesztel√©shez)\n",
    "scheduler.add_match(test_url, interval_minutes=1)\n",
    "\n",
    "# √útemez≈ë ind√≠t√°sa\n",
    "scheduler.start_scheduler()\n",
    "\n",
    "print(\"‚è∞ Scheduler started! It will scrape every 1 minutes.\")\n",
    "print(\"üí° Run the next cell to check results after a few minutes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:25:10,616 - INFO - Results saved to sofascore_stats_20250725_182510.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Current results:\n",
      "Total scraping results: 3\n",
      "\n",
      "üïê Latest timestamp: 2025-07-25T18:24:53.347017\n",
      "üíæ Results saved to: sofascore_stats_20250725_182510.csv\n",
      "\n",
      "üìã Sample results:\n",
      "               Time Home xG Away xG  Stats Count\n",
      "2025-07-25T18:23:52     N/A     N/A          354\n",
      "2025-07-25T18:24:52     N/A     N/A          354\n",
      "2025-07-25T18:24:53     N/A     N/A          354\n"
     ]
    }
   ],
   "source": [
    "# Eredm√©nyek ellen≈ërz√©se √©s ment√©se\n",
    "\n",
    "print(\"üìä Current results:\")\n",
    "print(f\"Total scraping results: {len(scheduler.results)}\")\n",
    "\n",
    "if scheduler.results:\n",
    "    latest = scheduler.get_latest_stats()\n",
    "    if latest:\n",
    "        print(f\"\\nüïê Latest timestamp: {latest.get('timestamp')}\")\n",
    "        if latest.get('xg_data'):\n",
    "            print(f\"‚öΩ Latest xG: {latest['xg_data']}\")\n",
    "    \n",
    "    # CSV ment√©s\n",
    "    csv_file = scheduler.save_results_to_csv()\n",
    "    print(f\"üíæ Results saved to: {csv_file}\")\n",
    "    \n",
    "    # Egyszer≈± DataFrame megjelen√≠t√©s\n",
    "    if len(scheduler.results) > 0:\n",
    "        sample_data = []\n",
    "        for result in scheduler.results[-3:]:  # Utols√≥ 3 eredm√©ny\n",
    "            sample_data.append({\n",
    "                'Time': result.get('timestamp', '')[:19],  # D√°tum/id≈ë r√∂vid√≠tve\n",
    "                'Home xG': result.get('xg_data', {}).get('home_xg', 'N/A'),\n",
    "                'Away xG': result.get('xg_data', {}).get('away_xg', 'N/A'),\n",
    "                'Stats Count': len(result.get('raw_stats', []))\n",
    "            })\n",
    "        \n",
    "        df_sample = pd.DataFrame(sample_data)\n",
    "        print(\"\\nüìã Sample results:\")\n",
    "        print(df_sample.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Deployment options printed above!\n"
     ]
    }
   ],
   "source": [
    "# Felh≈ë deployment seg√©dk√≥d\n",
    "\n",
    "\"\"\"\n",
    "üåê FELH≈ê DEPLOYMENT OPCI√ìK:\n",
    "\n",
    "1. GOOGLE COLAB:\n",
    "   - Ingyenes GPU/TPU access\n",
    "   - Jupyter notebook k√∂rnyezet\n",
    "   - Korl√°tozott fut√°si id≈ë (12-24 √≥ra)\n",
    "\n",
    "2. KAGGLE KERNELS:\n",
    "   - Heti 30 √≥ra GPU id≈ë\n",
    "   - Internet access korl√°tozott\n",
    "   - Notebook k√∂rnyezet\n",
    "\n",
    "3. GITHUB ACTIONS (Ingyenes tier):\n",
    "   - Cron job alap√∫ √ºtemez√©s\n",
    "   - 2000 perc/h√≥ limit\n",
    "   - Eredm√©nyek GitHub-ra ment√©se\n",
    "\n",
    "4. RAILWAY/RENDER (Ingyenes tier):\n",
    "   - 24/7 fut√°s\n",
    "   - Kis resource limit\n",
    "   - Web app form√°ban\n",
    "\n",
    "GitHub Actions workflow p√©lda (.github/workflows/scraper.yml):\n",
    "\n",
    "name: SofaScore Scraper\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '*/30 * * * *'  # 30 percenk√©nt\n",
    "  workflow_dispatch:\n",
    "\n",
    "jobs:\n",
    "  scrape:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: 3.9\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install requests beautifulsoup4 selenium pandas\n",
    "        sudo apt-get update\n",
    "        sudo apt-get install -y chromium-browser\n",
    "    - name: Run scraper\n",
    "      run: python scraper.py\n",
    "    - name: Commit results\n",
    "      run: |\n",
    "        git config --local user.email \"action@github.com\"\n",
    "        git config --local user.name \"GitHub Action\"\n",
    "        git add *.csv\n",
    "        git commit -m \"Update scraping results\" || exit 0\n",
    "        git push\n",
    "\"\"\"\n",
    "\n",
    "print(\"üöÄ Deployment options printed above!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 18:27:13,484 - INFO - Scheduler stopped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßπ Cleaning up...\n",
      "‚úÖ Cleanup completed!\n"
     ]
    }
   ],
   "source": [
    "# Cleanup √©s le√°ll√≠t√°s\n",
    "\n",
    "print(\"üßπ Cleaning up...\")\n",
    "scheduler.stop_scheduler()\n",
    "scraper.cleanup()\n",
    "print(\"‚úÖ Cleanup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ## üìù Haszn√°lati √∫tmutat√≥\n",
    "# \n",
    "# ### Alapvet≈ë haszn√°lat:\n",
    "# 1. Futtasd le a csomagtelep√≠t√©st √©s importokat\n",
    "# 2. √Åll√≠tsd be a `test_url` v√°ltoz√≥t a k√≠v√°nt SofaScore m√©rk≈ëz√©s URL-j√©vel\n",
    "# 3. Futtasd le a tesztel≈ë cell√°t\n",
    "# 4. Ind√≠tsd el az √ºtemezett scrapinget\n",
    "# \n",
    "# ### Testreszab√°si lehet≈ës√©gek:\n",
    "# - `interval_minutes`: Scraping gyakoris√°g m√≥dos√≠t√°sa\n",
    "# - `headless`: False √©rt√©kkel l√°that√≥ b√∂ng√©sz≈ë ablak\n",
    "# - `save_results_to_csv()`: Automatikus ment√©s be√°ll√≠t√°sa\n",
    "# \n",
    "# ### Hibaelh√°r√≠t√°s:\n",
    "# - Ha nem m≈±k√∂dik a requests m√≥dszer, a Selenium automatikusan √°tveszi\n",
    "# - Chrome driver automatikusan telep√ºl a webdriver-manager seg√≠ts√©g√©vel\n",
    "# - R√©szletes logok a hib√°k nyomon k√∂vet√©s√©hez\n",
    "# \n",
    "# ### Teljes√≠tm√©ny optimaliz√°l√°s:\n",
    "# - Headless m√≥d gyorsabb fut√°shoz\n",
    "# - Requests el≈ësz√∂r pr√≥b√°lkozik (gyorsabb)\n",
    "# - Selenium csak sz√ºks√©g eset√©n (megb√≠zhat√≥bb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
